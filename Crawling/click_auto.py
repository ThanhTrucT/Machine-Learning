import csv
import requests
import time
from time import sleep
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor

# C·∫•u h√¨nh Selenium
profile_path = r'C:\Users\Tran Thanh Truc\AppData\Local\Google\Chrome\User Data\Profile 1'
options = Options()
options.add_argument(f"user-data-dir={profile_path}")  
options.add_argument("--disable-blink-features=AutomationControlled")
options.add_experimental_option("excludeSwitches", ["enable-automation"])
options.add_experimental_option("useAutomationExtension", False)

# Kh·ªüi ƒë·ªông tr√¨nh duy·ªát
service = Service('./chromedriver.exe')
browser = webdriver.Chrome(service=service, options=options)
browser.get("https://thanhnien.vn/thoi-su/phap-luat.htm")
sleep(3)

# Bi·∫øn to√†n c·ª•c
news_2023 = []
visited_links = set()
cnt = 0
MAX_ARTICLES = 211
session = requests.Session()

# Th·ªùi gian t·ªëi ƒëa ƒë·ªÉ cu·ªôn nhanh (gi√¢y)
FAST_SCROLL_TIME = 10
start_time = time.time()  

# üéØ H√†m cu·ªôn nhanh li√™n t·ª•c trong m·ªôt kho·∫£ng th·ªùi gian
def fast_scroll():
    while time.time() - start_time <= FAST_SCROLL_TIME:
        browser.execute_script("window.scrollBy(0, 5000);")  
        sleep(2)  # Cu·ªôn nhanh kh√¥ng ki·ªÉm tra b√†i b√°o

# üéØ H√†m cu·ªôn ch·∫≠m
def smooth_scroll():
    browser.execute_script("window.scrollTo(0, document.body.scrollHeight);")
    sleep(2)  
    try:
        return browser.find_element(By.CSS_SELECTOR, ".list__center.view-more.list__viewmore")
    except:
        return None 

# üéØ H√†m l·∫•y ng√†y ƒëƒÉng b√†i b√°o
def fetch_article_date(link):
    try:
        res = session.get(link, timeout=3)  
        soup = BeautifulSoup(res.content, 'html.parser')
        date_tag = soup.find('div', {'data-role': 'publishdate'})
        return link if date_tag and "2025" in date_tag.text else None
    except:
        return None

# üöÄ Giai ƒëo·∫°n 1: CU·ªòN NHANH
print("üöÄ ƒêang cu·ªôn nhanh ƒë·ªÉ t·∫£i d·ªØ li·ªáu...")
fast_scroll()
print("‚úÖ Cu·ªôn nhanh ho√†n t·∫•t! B·∫Øt ƒë·∫ßu cu·ªôn ch·∫≠m...")

# üöÄ Giai ƒëo·∫°n 2: CU·ªòN CH·∫¨M & CHECK B√ÄI B√ÅO 2023
while cnt < MAX_ARTICLES:
    print(f"üîÑ Cu·ªôn trang l·∫ßn {cnt // 10 + 1}...")

    show_more = smooth_scroll()

    if show_more:
        browser.execute_script("arguments[0].click();", show_more)
        print("‚úÖ Nh·∫•n n√∫t 'Xem th√™m' th√†nh c√¥ng!")
        sleep(2)
    else:
        print("‚ö† Kh√¥ng t√¨m th·∫•y n√∫t 'Xem th√™m', d·ª´ng l·∫°i!")
        break

    # üîç L·∫•y danh s√°ch b√†i b√°o
    articles = browser.find_elements(By.CSS_SELECTOR, 'div.box-category-item')
    links = []

    for article in articles:
        try:
            title_tag = article.find_element(By.TAG_NAME, 'a')
            link = title_tag.get_attribute('href')
            if link and link not in visited_links:
                links.append(link)
                visited_links.add(link)
        except Exception as e:
            print(f"‚ö† L·ªói khi l·∫•y link b√†i b√°o: {e}")

    # üèéÔ∏è D√πng ƒëa lu·ªìng ƒë·ªÉ ki·ªÉm tra ng√†y b√†i b√°o
    with ThreadPoolExecutor(max_workers=5) as executor:
        results = list(executor.map(fetch_article_date, links))

    # üìå L·ªçc b√†i b√°o nƒÉm 2023
    for result in results:
        if result and cnt < MAX_ARTICLES:
            news_2023.append(result)
            print(f"üìå B√†i b√°o {cnt+1}: {result}")
            cnt += 1

# ƒê√≥ng tr√¨nh duy·ªát sau khi xong
browser.quit()

# üìù L∆∞u k·∫øt qu·∫£ v√†o CSV
with open("news_2023.csv", mode="w", newline="", encoding="utf-8") as f:
    writer = csv.writer(f)
    writer.writerow(["STT", "Link B√†i B√°o"])
    for index, link in enumerate(news_2023, start=1):
        writer.writerow([index, link])

print("\n‚úÖ ƒê√£ l∆∞u k·∫øt qu·∫£ v√†o file news_2023.csv th√†nh c√¥ng!")
